{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines"
      ],
      "metadata": {
        "id": "oeGmZomGwOmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vimos anteriormente, quando trabalhamos com Machine Learning, o ponto mais sensível do projeto é o *dataset*. Uma vez que tudo em um projeto de Machine Learning se baseiam nele.\n",
        "\n",
        "Para entender e interpretar o dataset podemos utilizar as seguintes etapas:\n",
        "\n",
        "1. Coletar o máximo de informações de metadados do dataset, tais como: descritivos e domínios das features, momento e contexto da coleta, etc.;\n",
        "\n",
        "1. Separar o dataset entre: conjunto de treino e conjunto de teste;\n",
        "\n",
        "1. Com o conjunto de treino:\n",
        "\n",
        "    i. Fazer uma EDA, Exploratory Data Analysis, que busque entender melhor as distribuições individuais de cada feature, assim bem como, buscar por possíveis correlações dentre elas;\n",
        "\n",
        "    ii. Com o suporte da EDA, selecionar as features candidatas para uso na construção do modelo;\n",
        "\n",
        "    iii. [Diagnosticar e Mitigar Anomalias](#diagnosticar-e-mitigar-anomalias):\n",
        "\n",
        "    iv. [Normalizar as Dimensões](#normalizar-as-dimensões);\n",
        "\n",
        "    v. Selecionar o modelo;\n",
        "\n",
        "    vi. Treinar o modelo;\n",
        "\n",
        "1. Com o conjunto de testes:\n",
        "\n",
        "    i. Verificar a perfomance de acordo com as métricas adequadas.\n",
        "\n",
        "    ii. Se necessário, voltar para o passo 3, e iterar sobre as etapas, até que se obtenha um modelo satisfatório.\n",
        "\n",
        "Nessa seção, vamos focar nas etapas 3.iii e 3.iv, ou seja, em como diagnosticar e mitigar anomalias, e em rever como normalizar as dimensões."
      ],
      "metadata": {
        "id": "8EJtcwPCzq3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hsandmann/biblio/refs/heads/main/ml/aula04/flow_pipeline.png\" width=\"60%\">"
      ],
      "metadata": {
        "id": "0fanVbr4i-mG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "kw0byaBiT9zQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ilustrar a seção, vamos trabalhar com o dataset [Titanic](https://www.kaggle.com/competitions/titanic/) da Kaggle. Esse dataset é composto por dados de passageiros do navio Titanic, e o objetivo é construir um modelo que seja capaz de prever se um passageiro sobreviveu ou não ao naufrágio, a partir de suas características, como idade, sexo, classe social, etc.\n",
        "\n",
        "A escolha desse dataset se deve ao fato de ele ser um dos mais clássicos e utilizados para fins educacionais, e por isso, já ter sido bastante explorado, o que nos permite focar nas etapas de pré-processamento, sem a necessidade de gastar muito tempo com a EDA, ou com a seleção de features."
      ],
      "metadata": {
        "id": "ZEdWTOLNUA4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataset é composto por 891 linhas, e 12 features, sendo elas:\n",
        "\n",
        "| Feature     | Descrição                                              | Tipo       | Missing?       | Uso típico                              |\n",
        "|:------------|:-------------------------------------------------------|:-----------|:--------------|:------------------------------------|\n",
        "| PassengerId | ID do passageiro                                       | int        | Não           | Ignorar ou índice                   |\n",
        "| Survived    | 0 = Não sobreviveu, 1 = Sobreviveu (target)            | int (0/1)  | Não           | Target                              |\n",
        "| Pclass      | Classe do bilhete (1 = alta, 2 = média, 3 = baixa)     | int        | Não           | Categórica ordinal                  |\n",
        "| Name        | Nome do passageiro                                     | string     | Não           | Feature engineering (títulos)       |\n",
        "| Sex         | Sexo (male / female)                                   | string     | Não           | Categórica → dummy                  |\n",
        "| Age         | Idade                                                  | float      | Sim (~20%)    | Imputar (média/mediana)             |\n",
        "| SibSp       | Nº de irmãos/cônjuges a bordo                          | int        | Não           | Numérica                            |\n",
        "| Parch       | Nº de pais/filhos a bordo                              | int        | Não           | Numérica                            |\n",
        "| Ticket      | Número do bilhete                                      | string     | Não           | Pode ignorar ou extrair info        |\n",
        "| Fare        | Tarifa paga                                            | float      | Poucos        | Numérica (escalar)                  |\n",
        "| Cabin       | Número da cabine                                       | string     | Muitos (~77%) | Ignorar ou extrair deck             |\n",
        "| Embarked    | Porto de embarque (C, Q, S)                            | string     | Poucos        | Categórica → dummy                  |\n"
      ],
      "metadata": {
        "id": "l-eoc9YdXmSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note que existem diversas informações faltantes, e que as features são de tipos variados, o que nos dá a oportunidade de explorar diversas técnicas de pré-processamento, e de construção de pipelines."
      ],
      "metadata": {
        "id": "yG2NZoLXYK44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separação Treino/Teste"
      ],
      "metadata": {
        "id": "GiVpUGnsYudN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse dataset já vem separado em um conjunto de treino, e um conjunto de teste, o que é ótimo, pois nos permite focar na construção do modelo, sem a necessidade de gastar tempo com a separação dos dados.\n",
        "\n",
        "Logo:\n",
        "\n",
        "1. Importar as bibliotecas necessárias;\n",
        "1. Carregar os datasets de treino e teste;\n",
        "1. Inspecionar os datasets, para verificar se estão carregados corretamente, e para ter uma ideia geral de como eles são."
      ],
      "metadata": {
        "id": "VbOXh8CDYvMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "url_train = \"https://raw.githubusercontent.com/hsandmann/biblio/refs/heads/main/ml/aula04/titanic/train.csv\"\n",
        "\n",
        "# Carregar os dados\n",
        "df = pd.read_csv(url_train)\n",
        "\n",
        "# Selecao de features e target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "YEKa9VgxYzMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecionando o Conjunto de Treino:"
      ],
      "metadata": {
        "id": "l4_-qLC2ZaPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "LyqoDFLtZeBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspeção dos Targets do Conjunto de Treino"
      ],
      "metadata": {
        "id": "jMAVdaYcavcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "Icr_Yao4auVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como é possível notar, o dataset possui uma grande variedade de anomalias, tais como: dados faltantes, dados ruidosos, e dados inconsistentes. Além disso, as features são de tipos variados, o que nos dá a oportunidade de explorar diversas técnicas de pré-processamento, e de construção de pipelines."
      ],
      "metadata": {
        "id": "iz5enn0zbRwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines"
      ],
      "metadata": {
        "id": "0vyEexnxklJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que precisamos realizar diversas etapas de [**pré-processamento**](https://scikit-learn.org/stable/modules/preprocessing.html), tais como:\n",
        "\n",
        "- imputação de dados faltantes,\n",
        "- codificação de variáveis categóricas,\n",
        "- normalização de features numéricas, etc.\n",
        "\n",
        "Para isso, podemos utilizar o conceito de pipelines, que nos permite encadear essas etapas de forma organizada e eficiente.\n",
        "\n",
        "Ainda, quando separmos o dataset entre treino e teste, é fundamental que o processo de pré-processamento seja aplicado de forma consistente em ambos os conjuntos, para evitar vazamento de dados (*data leakage*) e para garantir que o modelo seja avaliado de forma justa.\n",
        "\n",
        "Pipelines são uma forma de organizar o fluxo de trabalho de pré-processamento e modelagem, permitindo que as etapas sejam executadas de forma sequencial e que o código seja mais limpo e fácil de entender. Além disso, pipelines facilitam a reprodução do processo e a aplicação do mesmo processo em novos dados.\n",
        "\n",
        "Para ilustrar o conceito de pipelines, vamos utilizar a biblioteca `sklearn`, que possui uma classe chamada [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), que nos permite encadear as etapas de pré-processamento e modelagem de forma simples e eficiente."
      ],
      "metadata": {
        "id": "oM4Xpac3kngY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diagnosticar e Mitigar Anomalias"
      ],
      "metadata": {
        "id": "dVG3IaqiN-qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todo o dataset pode apresentar problemas de faltas de dados ou dados que não tem sentido dentro do contexto da feature no dataset. Esses dados são chamados de anomalias, e podem ser classificados em três tipos:\n",
        "\n",
        "1. Dados faltantes: quando uma feature possui valores ausentes, ou seja, não há informação disponível para aquela feature em determinada linha do dataset. Por exemplo, no dataset do Titanic, a feature `Age` possui cerca de 20% de dados faltantes.\n",
        "\n",
        "1. Dados ruidosos: quando uma feature possui valores que são claramente errados, ou que não fazem sentido dentro do contexto da feature. Por exemplo, no dataset do Titanic, a feature \"Age\" possui alguns valores negativos, o que não faz sentido, pois idade não pode ser negativa.\n",
        "\n",
        "1. Dados inconsistentes: quando uma feature possui valores que são contraditórios, ou que não seguem um padrão lógico. Por exemplo, no dataset do Titanic, a feature `Cabin` possui muitos valores faltantes, e os valores presentes são bastante variados, o que torna difícil extrair informações úteis dessa feature.\n",
        "\n",
        "A fim de lidar com essas anomalias, podemos utilizar diversas técnicas de pré-processamento, tais como:\n",
        "\n",
        "1. Imputação de dados faltantes: quando uma feature possui valores ausentes, podemos utilizar técnicas de imputação para preencher esses valores. Por exemplo, podemos utilizar a média ou a mediana para preencher os valores faltantes da feature \"Age\".\n",
        "\n",
        "1. Remoção de dados ruidosos: quando uma feature possui valores que são claramente errados, podemos optar por remover essas linhas do dataset, ou por corrigir os valores, se possível. Por exemplo, podemos remover as linhas onde a feature `Age` possui valores negativos.\n",
        "\n",
        "1. Tratamento de dados inconsistentes: quando uma feature possui valores que são contraditórios, ou que não seguem um padrão lógico, podemos optar por remover essa feature do dataset, ou por extrair informações úteis dela, se possível. Por exemplo, podemos optar por remover a feature `Cabin` do dataset, ou por extrair o deck da cabine, se essa informação estiver presente."
      ],
      "metadata": {
        "id": "ZFaBqs_ml9PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features Numéricas"
      ],
      "metadata": {
        "id": "Vi9f3CcGlx8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature `Age`"
      ],
      "metadata": {
        "id": "mB7Y_0cinFki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro, vamos imputar dados faltantes na coluna de `Age`, a fim de poder melhor trabalhar com tal feature.\n",
        "\n",
        "Inspecionando a coluna, podemos notar que existem valores `nan`, o que torno um desafio interpretar os dados faltantes.\n",
        "\n",
        "**IMPUTAR**: quando precisamos substituir valores no dataset."
      ],
      "metadata": {
        "id": "MJDyqYs7l1fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['Age'].head(-5)"
      ],
      "metadata": {
        "id": "pb6OCwTMm-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para mitigar isso, podemos:"
      ],
      "metadata": {
        "id": "p_xnvTITYLRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_age = df['Age'].fillna(df['Age'].median()) # aqui o pandas localizou todos os nan e trocou pela mediana\n",
        "clean_age.head(-5)"
      ],
      "metadata": {
        "id": "N0kEZ-8znocq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, imagine ter que fazer isso em diversas colunas, teríamos que fazer uma função a parte apenas para realizar o pré-processamento. Para isso que existe o `Pipeline` do `sklearn`, essa classe padroniza os formatos de funções para pré-processamentos."
      ],
      "metadata": {
        "id": "t8HT2PSXoWEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trabalhando com todas as features numéricas"
      ],
      "metadata": {
        "id": "kNUOBA6roY1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas numericas: imputar + escalar\n",
        "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # mediana eh robusta para Age\n",
        "])"
      ],
      "metadata": {
        "id": "M3iw4KvEoU48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vimos na aula passada, para valores numéricos, é importante uma normalização dos dados, logo, podemos deixar esse pipeline mais robusto:"
      ],
      "metadata": {
        "id": "p8IQoz4Do4IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features numericas: imputar + escalar\n",
        "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # mediana eh robusta para Age\n",
        "    ('scaler', StandardScaler())\n",
        "])"
      ],
      "metadata": {
        "id": "vKe4tHvcpAsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos aqui um pipeline na variável `numeric_tranformer` que tem duas transformações de pré-processamento em seu fluxo, sendo que:\n",
        "\n",
        "- **a primeira**: [imputa](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) valores da mediana para dados faltantes nas colunas numéricas selecionadas. Nota: poderia ser a média também.\n",
        "\n",
        "- **a segunda**: normaliza, [z-score](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), as features numéricas, logo agora todas estão na mesma ordem de grandeza."
      ],
      "metadata": {
        "id": "9Tbex_-1pFTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features Categóricas"
      ],
      "metadata": {
        "id": "ygKh79usqMqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O mesmo vale para as features categóricas, que podem apresentar dados faltantes também.\n",
        "\n",
        "Na mesma direção, podemos utilizar o `sklearn` para mitigar esse problema:"
      ],
      "metadata": {
        "id": "NTRmfUpdqaHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features categoricas: imputar + one-hot encoding\n",
        "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # moda\n",
        "    ('onehot', OneHotEncoder(sparse_output=False))  # converte para um vetor de zeros e uns\n",
        "])"
      ],
      "metadata": {
        "id": "qcJiqzj7qnlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, o pipeline também cria uma variável `categorical_transformer` com duas transformações sobre os dados categóricos, sendo:\n",
        "\n",
        "- **a primeira**: [imputa](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) valores da ocorrência de texto mais freqüente nos dados faltantes nas colunas categóricas selecionadas.\n",
        "\n",
        "- **a segunda**: transforma as entradas em um um vetor onde cada posição representa um valor único de ocorrências, eg.:\n",
        "\n",
        "    ``` plaintext\n",
        "    Antes:\n",
        "            cor tamanho  preco\n",
        "    0  vermelho       P  29.90\n",
        "    1      azul       M  49.90\n",
        "    2     verde       G  89.90\n",
        "    3  vermelho       M  45.00\n",
        "    4      azul       P  32.50\n",
        "\n",
        "    Nomes das novas colunas: ['cor_azul' 'cor_vermelho' 'tamanho_M' 'tamanho_P']\n",
        "\n",
        "    Depois (one-hot encoding):\n",
        "    preco  cor_azul  cor_vermelho  tamanho_M  tamanho_P\n",
        "    0  29.90       0.0           1.0        0.0        1.0\n",
        "    1  49.90       1.0           0.0        1.0        0.0\n",
        "    2  89.90       0.0           0.0        0.0        0.0\n",
        "    3  45.00       0.0           1.0        1.0        0.0\n",
        "    4  32.50       1.0           0.0        0.0        1.0\n",
        "    ```"
      ],
      "metadata": {
        "id": "jT3q73kzrms3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos tentar???\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'cor': ['vermelho', 'azul', 'verde', 'vermelho'],\n",
        "    'sexo': ['M', 'F', 'M', 'F']\n",
        "})\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "X = encoder.fit_transform(df)\n",
        "\n",
        "print(pd.DataFrame(X, columns=encoder.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "MupnlYwgt6gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pesquisando um pouco mais, existe uma outras classes: [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) e [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html).\n",
        "\n",
        "O que elas fazem? Quais as diferenças para o [OneHotEnconder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)? Exemplifique."
      ],
      "metadata": {
        "id": "eKvYzjjHuSGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hzhj5M7Vu40j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicando as Tranformações"
      ],
      "metadata": {
        "id": "rYroWidNvlZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Já vimos que é possível criar uma variável que representa uma pipeline para variáveis numéricas; assim bem como, uma outra variável que representa uma pipeline para variáveis categóricas.\n",
        "\n",
        "Vamos combinar todas as variáveis de pipelines e colocar em uma única variável de transformação, que será responsável por todo o pré-processamento dos dados, para isso, é prático utilizar a classe [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)."
      ],
      "metadata": {
        "id": "ZjE2RKmuvqWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comivar os pre-processadores com ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numerical', numeric_transformer, numeric_features),\n",
        "        ('categorical', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "xVSMCrmK27mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Pronto!!!**\n",
        "---"
      ],
      "metadata": {
        "id": "cSPXCijn4yfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos o pré-processamento definido, agora podemos acoplar o `ColumnTransforer` a um `Pipeline` e, então, aplicar sobre os dados e verificar a saída."
      ],
      "metadata": {
        "id": "q-D4LwIT45sF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhhDKxsJwJ8A"
      },
      "outputs": [],
      "source": [
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed = model.fit_transform(X_train)\n",
        "\n",
        "# pegando os nomes das novas colunas\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# resultados\n",
        "print(\"Shape após transformação:\", X_train_transformed.shape)\n",
        "print(\"Nomes das colunas geradas:\")\n",
        "print(list(feature_names))\n",
        "\n",
        "print(\"Dados transformados (primeiras 6 linhas, arredondado):\")\n",
        "print(np.round(X_train_transformed[:6], decimals=4))"
      ],
      "metadata": {
        "id": "WBAIVuoL5Nxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenando Processos"
      ],
      "metadata": {
        "id": "6QbaQLir5seJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image agora o pipeline perfeito, ele faz o pré-processamento e já, na seqüência, aplica o modelo. Isso parece muito bom!\n",
        "\n",
        "Vamos testar com nosso modelo conhecido? K-NN."
      ],
      "metadata": {
        "id": "z8y1OxjR5zKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────\n",
        "#  Modelo: KNN com k=3\n",
        "# ────────────────────────────────────────────────\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier',   KNeighborsClassifier(n_neighbors=3))\n",
        "])"
      ],
      "metadata": {
        "id": "a0cJ_I3a7SLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lapidado, nosso pipeline está pronto, vamos aplicar em nosso dataset:"
      ],
      "metadata": {
        "id": "QDDgzIsv7R0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# treinamento\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# previsoes\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Avaliação\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nResultados no conjunto de teste:\")\n",
        "print(f\"  Acurácia   = {acc:.3f}\")\n",
        "print(\"\\nPredições vs Real:\")\n",
        "print(\"Predito:  \", y_pred.tolist())\n",
        "print(\"Real:     \", y_test.tolist())"
      ],
      "metadata": {
        "id": "TPmU7jFG7nZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, todo o processo de está dentro de um pipeline, assim, é provável que erros de data leakage sejam minimizados. Também, o código é mais objeto e padronizado."
      ],
      "metadata": {
        "id": "AjvAvpso8PyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[^1]: https://scikit-learn.org/stable/data_transforms.html"
      ],
      "metadata": {
        "id": "ukrxEVRPrVZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1]: Sklearn - Dataset transformations at [https://scikit-learn.org/stable/data_transforms.html](https://scikit-learn.org/stable/data_transforms.html)."
      ],
      "metadata": {
        "id": "6MUXW8ogrXu2"
      }
    }
  ]
}